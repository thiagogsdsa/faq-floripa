from embeddings import FAQEmbedder
from preprocessor import Preprocessor
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from embeddings import *
#

from abc import ABC, abstractmethod

class BaseFAQRetriever(ABC):
    @abstractmethod
    def retrieve(self, question: str, top_k: int = 3) -> list:
        """
        Given a question, return the top_k most similar FAQs.
        Each result must be a dictionary containing:
            - 'id': index of the FAQ
            - 'question': original question text
            - 'answer': corresponding answer
            - 'score': similarity score
            - 'language': used language
        """
        pass


class FAQRetriever(BaseFAQRetriever):
    def __init__(self, faq_texts, language='pt', model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        # If language is None, use both pt and en
        if language is None:
            self.languages = ('pt', 'en')
        else:
            if isinstance(language, str):
                self.languages = (language,)
            else:
                raise ValueError("language must be 'pt', 'en', or None")

        # Create one FAQEmbedder per language
        self.embedders = {
            lang: FAQEmbedder(faq_texts, lang, model_name)
            for lang in self.languages
        }

    def retrieve(self, question, top_k=3, language):
        all_results = []
        langs_to_search = [language] if language else self.languages

        for lang in langs_to_search:
            embedder = self.embedders[lang]
            cleaned_question = embedder.preprocessor.clean(question)
            query_vec = embedder.model.encode([cleaned_question], convert_to_numpy=True)
            faiss.normalize_L2(query_vec)

            scores, indices = embedder.index.search(query_vec, top_k)

            for idx, score in zip(indices[0], scores[0]):
                all_results.append({
                    "id": embedder.ids[idx],
                    "question": embedder.texts[idx],
                    "answer": embedder.answers[idx],
                    "language": lang,
                    "score": float(score)
                })

        all_results.sort(key=lambda x: x['score'], reverse=True)
        return all_results[:top_k]



#    def retrieve(self, question, top_k=3):
#        all_results = []
#
#        for lang, embedder in self.embedders.items():
#            cleaned_question = embedder.preprocessor.clean(question)
#            query_vec = embedder.model.encode([cleaned_question], convert_to_numpy=True)
#            faiss.normalize_L2(query_vec)
#
#            scores, indices = embedder.index.search(query_vec, top_k)
#
#            for idx, score in zip(indices[0], scores[0]):
#                all_results.append({
#                    "id": embedder.ids[idx],
#                    "question": embedder.texts[idx],
#                    "answer": embedder.answers[idx],
#                    "language": lang,
#                    "score": float(score)
#                })
#
#        # Sort all results by score descending
#        all_results.sort(key=lambda x: x['score'], reverse=True)
#
#        # Return top_k results overall
#        return all_results[:top_k]

#class FAQRetriever(BaseFAQRetriever):
#    def __init__(self, faq_texts, language='pt', model_name='paraphrase-multilingual-MiniLM-L12-v2'):
#        self.language = language
#        self.embedder = FAQEmbedder(faq_texts, language, model_name)
#
#    def retrieve(self, question, top_k=3, language=None):
#        # Use the passed language or fallback to the default
#        lang = language or self.language
#
#        # Optionally, if your FAQEmbedder supports dynamic language switching,
#        # you might want to recreate the embedder or have a method to switch language.
#        # For now, let's assume embedder is fixed, so:
#        if lang != self.language:
#            raise ValueError(f"Language '{lang}' not supported by this embedder instance. Default is '{self.language}'.")
#
#        cleaned_question = self.embedder.preprocessor.clean(question)
#        query_vec = self.embedder.model.encode([cleaned_question], convert_to_numpy=True)
#        faiss.normalize_L2(query_vec)
#        scores, indices = self.embedder.index.search(query_vec, top_k)
#
#        results = []
#        for idx, score in zip(indices[0], scores[0]):
#            results.append({
#                "id": idx,
#                "question": self.embedder.texts[idx],
#                "answer": self.embedder.answers[idx],
#                "score": float(score),
#                "language": self.language
#            })
#        return results
#
#class FAQRetriever(BaseFAQRetriever):
#    def __init__(self, faq_texts, language='pt', model_name='paraphrase-multilingual-MiniLM-L12-v2'):
#        self.language = language
#        self.embedder = FAQEmbedder(faq_texts, language, model_name)
#
#    def retrieve(self, question, top_k=3):
#        cleaned_question = self.embedder.preprocessor.clean(question)
#        query_vec = self.embedder.model.encode([cleaned_question], convert_to_numpy=True)
#        faiss.normalize_L2(query_vec)
#        scores, indices = self.embedder.index.search(query_vec, top_k)
#
#        results = []
#        for idx, score in zip(indices[0], scores[0]):
#            results.append({
#                "id": idx,
#                "question": self.embedder.texts[idx],
#                "answer": self.embedder.answers[idx],
#                "score": float(score),
#                "language": self.language
#            })
#        return results


class FAQRetrieverTFIDF(BaseFAQRetriever):
    def __init__(self, faq_texts, language='pt'):
        self.faq_texts = faq_texts
        self.language = language

        self.preproc_pt = Preprocessor(language='pt')
        self.preproc_en = Preprocessor(language='en')

        self.questions_pt = [self.preproc_pt.clean(faq.get('question_pt', '')) for faq in faq_texts]
        self.questions_en = [self.preproc_en.clean(faq.get('question_en', '')) for faq in faq_texts]

        self.vectorizer_pt = TfidfVectorizer()
        self.vectorizer_en = TfidfVectorizer()

        self.tfidf_matrix_pt = self.vectorizer_pt.fit_transform(self.questions_pt)
        self.tfidf_matrix_en = self.vectorizer_en.fit_transform(self.questions_en)

    def retrieve(self, question, top_k=3):
        def search_in_language(lang):
            preproc = self.preproc_pt if lang == 'pt' else self.preproc_en
            vectorizer = self.vectorizer_pt if lang == 'pt' else self.vectorizer_en
            tfidf_matrix = self.tfidf_matrix_pt if lang == 'pt' else self.tfidf_matrix_en

            cleaned_question = preproc.clean(question)
            question_vec = vectorizer.transform([cleaned_question])
            similarities = cosine_similarity(question_vec, tfidf_matrix).flatten()

            results_lang = []
            for i, score in enumerate(similarities):
                faq = self.faq_texts[i]
                answer = faq.get(f'answer_{lang}', '')
                question_text = faq.get(f'question_{lang}', '')
                results_lang.append({
                    "id": i,
                    "question": question_text,
                    "answer": answer,
                    "score": float(score),
                    "language": lang
                })
            return results_lang

        # Retrieve in both languages and merge
        results_pt = search_in_language('pt')
        results_en = search_in_language('en')
        combined = sorted(results_pt + results_en, key=lambda x: x['score'], reverse=True)[:top_k]

        return combined


#class FAQRetriever:
#    def __init__(self, faq_texts, language='pt', model_name='paraphrase-multilingual-MiniLM-L12-v2'):
#        self.embedder = FAQEmbedder(faq_texts, language, model_name)
#
#    def retrieve(self, question, top_k=3):
#        cleaned_question = self.embedder.preprocessor.clean(question)
#        query_vec = self.embedder.model.encode([cleaned_question], convert_to_numpy=True)
#        faiss.normalize_L2(query_vec)
#        scores, indices = self.embedder.index.search(query_vec, top_k)
#        
#        results = []
#        for idx, score in zip(indices[0], scores[0]):
#            results.append({
#                "id": self.embedder.ids[idx],
#                "question": self.embedder.texts[idx],
#                "answer": self.embedder.answers[idx],
#                "score": float(score)
#            })
#        return results
#
#
#class FAQRetrieverTFIDF:
#    def __init__(self, faq_texts, language='pt' ):
#        self.faq_texts = faq_texts
#
#        # Cria dois preprocessadores
#        self.preproc_pt = Preprocessor(language='pt')
#        self.preproc_en = Preprocessor(language='en')
#
#        # Preprocessa e extrai as perguntas (PT e EN)
#        self.questions_pt = [self.preproc_pt.clean(faq.get('question_pt', '')) for faq in faq_texts]
#        self.questions_en = [self.preproc_en.clean(faq.get('question_en', '')) for faq in faq_texts]
#                # Cria dois vetorizadores distintos
#        self.vectorizer_pt = TfidfVectorizer()
#        self.vectorizer_en = TfidfVectorizer()
#
#        # Cria matrizes de TF-IDF
#        self.tfidf_matrix_pt = self.vectorizer_pt.fit_transform(self.questions_pt)
#        self.tfidf_matrix_en = self.vectorizer_en.fit_transform(self.questions_en)
#
#
#    def retrieve(self, question, language=None, top_k=3):
#        def search_in_language(lang):
#            preproc = self.preproc_pt if lang == 'pt' else self.preproc_en
#            vectorizer = self.vectorizer_pt if lang == 'pt' else self.vectorizer_en
#            tfidf_matrix = self.tfidf_matrix_pt if lang == 'pt' else self.tfidf_matrix_en
#
#            cleaned_question = preproc.clean(question)
#            question_vec = vectorizer.transform([cleaned_question])
#            similarities = cosine_similarity(question_vec, tfidf_matrix).flatten()
#
#            results_lang = []
#            for i, score in enumerate(similarities):
#                answer = self.faq_texts[i]['answer_pt'] if lang == 'pt' else self.faq_texts[i]['answer_en']
#                results_lang.append({'answer': answer, 'score': score, 'language': lang})
#            return results_lang
#
#        results = []
#
#        if language is None:
#            # Search both languages and merge
#            results.extend(search_in_language('pt'))
#            results.extend(search_in_language('en'))
#            # Sort combined results by score
#            results = sorted(results, key=lambda x: x['score'], reverse=True)[:top_k]
#
#        elif language in {'pt', 'en'}:
#            # Search only the specified language
#            results = search_in_language(language)
#            # Sort and get top_k
#            results = sorted(results, key=lambda x: x['score'], reverse=True)[:top_k]
#
#        return results


